{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(filepath_or_buffer='./communities.data', sep=',', header=None, na_values='?', keep_default_na=True)\n",
    "column_names = ['state', 'county', 'community', 'communityname', 'fold', 'population', 'householdsize', 'racepctblack', 'racePctWhite', 'racePctAsian', 'racePctHisp', 'agePct12t21', 'agePct12t29', 'agePct16t24', 'agePct65up', 'numbUrban', 'pctUrban', 'medIncome', 'pctWWage', 'pctWFarmSelf', 'pctWInvInc', 'pctWSocSec', 'pctWPubAsst', 'pctWRetire', 'medFamInc', 'perCapInc', 'whitePerCap', 'blackPerCap', 'indianPerCap', 'AsianPerCap', 'OtherPerCap', 'HispPerCap', 'NumUnderPov', 'PctPopUnderPov', 'PctLess9thGrade', 'PctNotHSGrad', 'PctBSorMore', 'PctUnemployed', 'PctEmploy', 'PctEmplManu', 'PctEmplProfServ', 'PctOccupManu', 'PctOccupMgmtProf', 'MalePctDivorce', 'MalePctNevMarr', 'FemalePctDiv', 'TotalPctDiv', 'PersPerFam', 'PctFam2Par', 'PctKids2Par', 'PctYoungKids2Par', 'PctTeen2Par', 'PctWorkMomYoungKids', 'PctWorkMom', 'NumIlleg', 'PctIlleg', 'NumImmig', 'PctImmigRecent', 'PctImmigRec5', 'PctImmigRec8', 'PctImmigRec10', 'PctRecentImmig', 'PctRecImmig5', 'PctRecImmig8', 'PctRecImmig10', 'PctSpeakEnglOnly', 'PctNotSpeakEnglWell', 'PctLargHouseFam', 'PctLargHouseOccup', 'PersPerOccupHous', 'PersPerOwnOccHous', 'PersPerRentOccHous', 'PctPersOwnOccup', 'PctPersDenseHous', 'PctHousLess3BR', 'MedNumBR', 'HousVacant', 'PctHousOccup', 'PctHousOwnOcc', 'PctVacantBoarded', 'PctVacMore6Mos', 'MedYrHousBuilt', 'PctHousNoPhone', 'PctWOFullPlumb', 'OwnOccLowQuart', 'OwnOccMedVal', 'OwnOccHiQuart', 'RentLowQ', 'RentMedian', 'RentHighQ', 'MedRent', 'MedRentPctHousInc', 'MedOwnCostPctInc', 'MedOwnCostPctIncNoMtg', 'NumInShelters', 'NumStreet', 'PctForeignBorn', 'PctBornSameState', 'PctSameHouse85', 'PctSameCity85', 'PctSameState85', 'LemasSwornFT', 'LemasSwFTPerPop', 'LemasSwFTFieldOps', 'LemasSwFTFieldPerPop', 'LemasTotalReq', 'LemasTotReqPerPop', 'PolicReqPerOffic', 'PolicPerPop', 'RacialMatchCommPol', 'PctPolicWhite', 'PctPolicBlack', 'PctPolicHisp', 'PctPolicAsian', 'PctPolicMinor', 'OfficAssgnDrugUnits', 'NumKindsDrugsSeiz', 'PolicAveOTWorked', 'LandArea', 'PopDens', 'PctUsePubTrans', 'PolicCars', 'PolicOperBudg', 'LemasPctPolicOnPatr', 'LemasGangUnitDeploy', 'LemasPctOfficDrugUn', 'PolicBudgPerPop', 'ViolentCrimesPerPop']\n",
    "df.columns = column_names\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do GBR, Ridge, Lasso on all 122 variables (same as last week)\n",
    "y = df['ViolentCrimesPerPop'].copy()\n",
    "X = df.drop(\n",
    "    columns=['state', 'county', 'community', 'communityname', 'fold', 'ViolentCrimesPerPop']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lasso\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import cross_validate, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "\n",
    "grid = {\n",
    "    'estimator__alpha': np.logspace(-3, 1, 100)\n",
    "}\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('imputer', SimpleImputer(missing_values=np.nan, strategy='median')),\n",
    "    ('standardise', StandardScaler()),\n",
    "    ('estimator', Lasso())\n",
    "])\n",
    "\n",
    "# Note: The GridSearchCV only sees 2/3 of the data (each iteration), and further divides this into 1/3 and 2/3\n",
    "# for hyperparameter optimisation\n",
    "\n",
    "grid_search_cv = GridSearchCV(estimator=pipe, param_grid=grid, cv=3)\n",
    "\n",
    "# Note: by doing the gridsearchCV within the cross-validate function –  this is how we’re doing the nestedCV\n",
    "# This is different to what we did last week - because last week we only split the data once (we do it three times here)\n",
    "# The 'cross_validate' function splits the data in test (1/3) and train (2/3), \n",
    "# then passes the training set on to GridSearchCV to do it's own hyperparmater optimisation (using CV). \n",
    "# This is repeated 3 times.\n",
    "\n",
    "# The cv_results in:\n",
    "# test_score = cv_result['test_score'] is based on the R-squared using the best model from the \n",
    "# GridSearchCV on the data initially held out (33% of the imputed data). (3 values)\n",
    "# train_score = cv_result['train_score'] is based on the R-squared from the best model from the\n",
    "# GridSearchCV but on the data used by the GridSearchCV (66% of the imputed (for missing) data)\n",
    " \n",
    "\n",
    "cv_result = cross_validate(estimator=grid_search_cv, X=X, y=y, cv=3, return_train_score=True)\n",
    "test_score = cv_result['test_score']\n",
    "train_score = cv_result['train_score']\n",
    "plt.bar(x=range(2), \n",
    "        height = [np.mean(train_score), np.mean(test_score)], \n",
    "        tick_label=['Train', 'Test'], \n",
    "        yerr=[np.std(train_score), np.std(test_score)])\n",
    "plt.title('Lasso CV')\n",
    "plt.show()\n",
    "\n",
    "_ = grid_search_cv.fit(X=X, y=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now check the predictions from our Lasso\n",
    "y_pred = grid_search_cv.predict(X=X)\n",
    "pred_df = pd.DataFrame(data=y_pred, columns=['Lasso'], index=X.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ridge\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "grid = {\n",
    "    'estimator__alpha': np.logspace(-3, 2, 100)\n",
    "}\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('imputer', SimpleImputer(missing_values=np.nan, strategy='median')),\n",
    "    ('standardise', StandardScaler()),\n",
    "    ('estimator', Ridge())\n",
    "])\n",
    "\n",
    "grid_search_cv = GridSearchCV(estimator=pipe, param_grid=grid, cv=3)\n",
    "cv_result = cross_validate(grid_search_cv, X=X, y=y, cv=3, return_train_score=True)\n",
    "test_score = cv_result['test_score']\n",
    "train_score = cv_result['train_score']\n",
    "plt.bar(x=range(2), height = [np.mean(train_score), np.mean(test_score)], tick_label=['Train', 'Test'], \n",
    "        yerr=[np.std(train_score), np.std(test_score)])\n",
    "plt.title('Ridge CV')\n",
    "plt.show()\n",
    "\n",
    "grid_search_cv.fit(X=X, y=y)\n",
    "y_pred = grid_search_cv.predict(X=X)\n",
    "pred_df['Ridge'] = y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GBR\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "grid = {\n",
    "    # 'estimator__learning_rate': np.logspace(-2, 1, 3),\n",
    "    'estimator__learning_rate': np.logspace(-2, -0.5, 3),\n",
    "    'estimator__min_samples_split': np.logspace(-2, 0, 3),\n",
    "    'estimator__max_depth': np.arange(1, 5, 2)\n",
    "}\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('imputer', SimpleImputer()),\n",
    "    ('standardise', StandardScaler()),\n",
    "    ('estimator', GradientBoostingRegressor(random_state=42))\n",
    "])\n",
    "\n",
    "grid_search_cv = GridSearchCV(estimator=pipe, param_grid=grid, cv=3)\n",
    "cv_result = cross_validate(estimator=grid_search_cv, X=X, y=y, cv=3, return_train_score=True)\n",
    "test_score = cv_result['test_score']\n",
    "train_score = cv_result['train_score']\n",
    "plt.bar(x=range(2), \n",
    "        height = [np.mean(train_score), np.mean(test_score)], \n",
    "        tick_label=['Train', 'Test'], \n",
    "        yerr=[np.std(train_score), np.std(test_score)])\n",
    "plt.title('GBR CV')\n",
    "plt.show()\n",
    "\n",
    "grid_search_cv.fit(X=X, y=y)\n",
    "pred = grid_search_cv.predict(X=X)\n",
    "pred_df['GBR'] = pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now drop variables involving race\n",
    "# racepctblack: percentage of population that is african american (numeric - decimal)\n",
    "# racePctWhite: percentage of population that is caucasian (numeric - decimal)\n",
    "# racePctAsian: percentage of population that is of asian heritage (numeric - decimal)\n",
    "# racePctHisp: percentage of population that is of hispanic heritage (numeric - decimal)\n",
    "# whitePerCap: per capita income for caucasians (numeric - decimal)\n",
    "# blackPerCap: per capita income for african americans (numeric - decimal)\n",
    "# indianPerCap: per capita income for native americans (numeric - decimal)\n",
    "# AsianPerCap: per capita income for people with asian heritage (numeric - decimal)\n",
    "# OtherPerCap: per capita income for people with 'other' heritage (numeric - decimal)\n",
    "# HispPerCap: per capita income for people with hispanic heritage (numeric - decimal)\n",
    " \n",
    "race_variables = ['racepctblack', 'racePctWhite', 'racePctAsian', 'racePctHisp', 'whitePerCap', 'blackPerCap', 'indianPerCap',\n",
    "                  'AsianPerCap', 'OtherPerCap', 'HispPerCap']\n",
    "\n",
    "X_wo_race = X.drop(columns=race_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lasso w/o race variables\n",
    "grid = {\n",
    "    'estimator__alpha': np.logspace(-3, 1, 100)\n",
    "}\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('imputer', SimpleImputer(missing_values=np.nan, strategy='median')),\n",
    "    ('standardise', StandardScaler()),\n",
    "    ('estimator', Lasso())\n",
    "])\n",
    "\n",
    "grid_search_cv = GridSearchCV(estimator=pipe, param_grid=grid, cv=3)\n",
    "cv_result = cross_validate(estimator=grid_search_cv, X=X_wo_race, y=y, cv=3, return_train_score=True)\n",
    "test_score = cv_result['test_score']\n",
    "train_score = cv_result['train_score']\n",
    "plt.bar(x=range(2), \n",
    "        height = [np.mean(train_score), np.mean(test_score)], \n",
    "        tick_label=['Train', 'Test'], \n",
    "        yerr=[np.std(train_score), np.std(test_score)])\n",
    "plt.title('Lasso CV')\n",
    "plt.show()\n",
    "\n",
    "grid_search_cv.fit(X=X_wo_race, y=y)\n",
    "pred = grid_search_cv.predict(X=X_wo_race)\n",
    "pred_df['Lasso_wo_race'] = pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ridge w/o race variables\n",
    "grid = {\n",
    "    'estimator__alpha': np.logspace(-3, 2, 100)\n",
    "}\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('imputer', SimpleImputer(missing_values=np.nan, strategy='median')),\n",
    "    ('standardise', StandardScaler()),\n",
    "    ('estimator', Ridge())\n",
    "])\n",
    "\n",
    "grid_search_cv = GridSearchCV(estimator=pipe, param_grid=grid, cv=3)\n",
    "cv_result = cross_validate(grid_search_cv, X=X_wo_race, y=y, cv=3, return_train_score=True)\n",
    "test_score = cv_result['test_score']\n",
    "train_score = cv_result['train_score']\n",
    "plt.bar(x=range(2), height = [np.mean(train_score), np.mean(test_score)], tick_label=['Train', 'Test'], \n",
    "        yerr=[np.std(train_score), np.std(test_score)])\n",
    "plt.title('Ridge CV')\n",
    "plt.show()\n",
    "\n",
    "grid_search_cv.fit(X=X_wo_race, y=y)\n",
    "pred = grid_search_cv.predict(X=X_wo_race)\n",
    "pred_df['Ridge_wo_race'] = pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GBR\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "grid = {\n",
    "    # 'estimator__learning_rate': np.logspace(-2, 1, 3),\n",
    "    'estimator__learning_rate': np.logspace(-2, -0.5, 3),\n",
    "    'estimator__min_samples_split': np.logspace(-2, 0, 3),\n",
    "    'estimator__max_depth': np.arange(1, 5, 2)\n",
    "}\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('imputer', SimpleImputer()),\n",
    "    ('standardise', StandardScaler()),\n",
    "    ('estimator', GradientBoostingRegressor(random_state=42))\n",
    "])\n",
    "\n",
    "grid_search_cv = GridSearchCV(estimator=pipe, param_grid=grid, cv=3)\n",
    "cv_result = cross_validate(estimator=grid_search_cv, X=X_wo_race, y=y, cv=3, return_train_score=True)\n",
    "test_score = cv_result['test_score']\n",
    "train_score = cv_result['train_score']\n",
    "plt.bar(x=range(2), \n",
    "        height = [np.mean(train_score), np.mean(test_score)], \n",
    "        tick_label=['Train', 'Test'], \n",
    "        yerr=[np.std(train_score), np.std(test_score)])\n",
    "plt.title('GBR CV')\n",
    "plt.show()\n",
    "\n",
    "grid_search_cv.fit(X=X_wo_race, y=y)\n",
    "pred = grid_search_cv.predict(X=X_wo_race)\n",
    "pred_df['GBR_wo_race'] = pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the top 10 predicted communities with each method\n",
    "cities_df = df['communityname']\n",
    "cities_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_50_index = y.sort_values(ascending=False)[:50].index\n",
    "\n",
    "width = 0.1\n",
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(10, 20)\n",
    "\n",
    "plt.barh(y=np.arange(len(top_50_index)), width=y[top_50_index], height=width, label='Actual')\n",
    "\n",
    "for idx, item in enumerate(pred_df.columns):\n",
    "    plt.barh(y=np.arange(len(top_50_index)) + ((idx + 1 ) * width), \n",
    "             width=pred_df[item][top_50_index], \n",
    "             height=width,\n",
    "             label=item)\n",
    "\n",
    "ax.set_yticks(ticks=np.arange(len(top_50_index)) + 0.2,\n",
    "              labels=cities_df[top_50_index])\n",
    "ax.invert_yaxis()\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at the correlation between 'race' related variables and other variables\n",
    "pd.options.display.max_rows = None\n",
    "X.corr()[race_variables].drop(race_variables)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
