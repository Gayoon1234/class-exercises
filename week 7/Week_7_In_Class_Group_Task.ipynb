{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "666c6f73",
      "metadata": {
        "id": "666c6f73"
      },
      "outputs": [],
      "source": [
        "# read in the data\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "#from google.colab import files  # uncomment if using colab\n",
        "#uploades = files.upload()  # uncomment if using colab\n",
        "df = pd.read_csv(filepath_or_buffer='./communities.data', sep=',', header=None, na_values='?', keep_default_na=True)\n",
        "column_names = ['state', 'county', 'community', 'communityname', 'fold', 'population', 'householdsize', 'racepctblack', 'racePctWhite', 'racePctAsian', 'racePctHisp', 'agePct12t21', 'agePct12t29', 'agePct16t24', 'agePct65up', 'numbUrban', 'pctUrban', 'medIncome', 'pctWWage', 'pctWFarmSelf', 'pctWInvInc', 'pctWSocSec', 'pctWPubAsst', 'pctWRetire', 'medFamInc', 'perCapInc', 'whitePerCap', 'blackPerCap', 'indianPerCap', 'AsianPerCap', 'OtherPerCap', 'HispPerCap', 'NumUnderPov', 'PctPopUnderPov', 'PctLess9thGrade', 'PctNotHSGrad', 'PctBSorMore', 'PctUnemployed', 'PctEmploy', 'PctEmplManu', 'PctEmplProfServ', 'PctOccupManu', 'PctOccupMgmtProf', 'MalePctDivorce', 'MalePctNevMarr', 'FemalePctDiv', 'TotalPctDiv', 'PersPerFam', 'PctFam2Par', 'PctKids2Par', 'PctYoungKids2Par', 'PctTeen2Par', 'PctWorkMomYoungKids', 'PctWorkMom', 'NumIlleg', 'PctIlleg', 'NumImmig', 'PctImmigRecent', 'PctImmigRec5', 'PctImmigRec8', 'PctImmigRec10', 'PctRecentImmig', 'PctRecImmig5', 'PctRecImmig8', 'PctRecImmig10', 'PctSpeakEnglOnly', 'PctNotSpeakEnglWell', 'PctLargHouseFam', 'PctLargHouseOccup', 'PersPerOccupHous', 'PersPerOwnOccHous', 'PersPerRentOccHous', 'PctPersOwnOccup', 'PctPersDenseHous', 'PctHousLess3BR', 'MedNumBR', 'HousVacant', 'PctHousOccup', 'PctHousOwnOcc', 'PctVacantBoarded', 'PctVacMore6Mos', 'MedYrHousBuilt', 'PctHousNoPhone', 'PctWOFullPlumb', 'OwnOccLowQuart', 'OwnOccMedVal', 'OwnOccHiQuart', 'RentLowQ', 'RentMedian', 'RentHighQ', 'MedRent', 'MedRentPctHousInc', 'MedOwnCostPctInc', 'MedOwnCostPctIncNoMtg', 'NumInShelters', 'NumStreet', 'PctForeignBorn', 'PctBornSameState', 'PctSameHouse85', 'PctSameCity85', 'PctSameState85', 'LemasSwornFT', 'LemasSwFTPerPop', 'LemasSwFTFieldOps', 'LemasSwFTFieldPerPop', 'LemasTotalReq', 'LemasTotReqPerPop', 'PolicReqPerOffic', 'PolicPerPop', 'RacialMatchCommPol', 'PctPolicWhite', 'PctPolicBlack', 'PctPolicHisp', 'PctPolicAsian', 'PctPolicMinor', 'OfficAssgnDrugUnits', 'NumKindsDrugsSeiz', 'PolicAveOTWorked', 'LandArea', 'PopDens', 'PctUsePubTrans', 'PolicCars', 'PolicOperBudg', 'LemasPctPolicOnPatr', 'LemasGangUnitDeploy', 'LemasPctOfficDrugUn', 'PolicBudgPerPop', 'ViolentCrimesPerPop']\n",
        "df.columns = column_names\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0c3d82a2",
      "metadata": {
        "id": "0c3d82a2"
      },
      "outputs": [],
      "source": [
        "# sanity check - we should have 1994 rows and 128 columns\n",
        "\n",
        "print(f'There are {len(df)} rows')\n",
        "print(f'and {len(df.columns)} columns')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c60c475a",
      "metadata": {
        "id": "c60c475a"
      },
      "outputs": [],
      "source": [
        "#drop useless variables/features in the data\n",
        "\n",
        "y = df['ViolentCrimesPerPop'].copy()\n",
        "X = df.drop(\n",
        "    columns=['state', 'county', 'community', 'communityname', 'fold', 'ViolentCrimesPerPop']\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7b4ea5a4",
      "metadata": {
        "id": "7b4ea5a4"
      },
      "outputs": [],
      "source": [
        "# Now we'll take a quick look at the variables\n",
        "# we'll use include='all' to show all columns, not just numeric ones\n",
        "pd.options.display.max_columns = None\n",
        "df.describe(include='all')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "16c3d10e",
      "metadata": {
        "id": "16c3d10e"
      },
      "outputs": [],
      "source": [
        "## Let's look at our dependent variable - ViolentCrimesPerPop\n",
        "df['ViolentCrimesPerPop'].describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "40f3cb24",
      "metadata": {
        "id": "40f3cb24"
      },
      "outputs": [],
      "source": [
        "# Group 1: OLS\n",
        "# Group 2: LASSO\n",
        "\n",
        "# Task: break into two groups. Group 1 will present OLS results for the\n",
        "# prediction question. Then group 1 will switch and present LASSO for the\n",
        "# causal question. Group 2 argues for LASSO (prediction question) and \n",
        "# for OLS (causal question)\n",
        "\n",
        "# Brief: The government needs advice on how to reduce crime in the country.\n",
        "\n",
        "# They have a budget of $1million. \n",
        "# This means they have limitted resources and must be selective with how to\n",
        "# use their resources. \n",
        "\n",
        "# The govt has two questions they want answered:\n",
        "\n",
        "# A) Prediction Question: Which areas should they target with efforts to \n",
        "#     reduce crime?\n",
        "#     Be clear about: the characteristics of the areas to target\n",
        "\n",
        "# B) Causal Question: What interventions should they employ to reduce crime?\n",
        "\n",
        "\n",
        "# They put out a tender for this policy advice.\n",
        "# Two consulting firms compete to win the contract.\n",
        "\n",
        "# Consulting firm one chose to use OLS for their model.\n",
        "# Consulting firm two chose to use LASSO for their model.\n",
        "\n",
        "# The judge will be the \"govt\" and they will choose to award the $1m contract to the \n",
        "# consulting firm who provides the \"best\" policy advice\n",
        "\n",
        "# Your job: to convince the judge why your consulting firm's model is the best\n",
        "\n",
        "# Details of what to present: argue how your model helps to solve the \n",
        "# a) prediction problem and b) a causal problem \n",
        "#    argue why your model is the best for these two purposes (and why other models are problematic)\n",
        "#     you have permission to play devils advocate \n",
        "#  For prediction: \n",
        "#    1) Describe the model that you used (features you started out with and included in model)\n",
        "#    2) How did you decide which variables to include in the model?\n",
        "#    3) How did you create a prediction model (e.g. type of CV procedure? Gridsearch range?)\n",
        "#    4) What are the in-sample and out of sample score for the prediction models\n",
        "#    5) What are the 'important features' that come out of your model?\n",
        "#    6) Explain how you will use the model predictions to solve violent crime\n",
        "#    think about: how will you use prediction scores? Use feature importance\n",
        "#     results? \n",
        "#  For causal estimation: \n",
        "#    1) Describe the model that you used (features you started out with and included in model)\n",
        "#    2) How did you decide which variables to include in the causal model?\n",
        "#    3) How did you create a causal model (did you regularise? why and why not \n",
        "#        e.g. sparse model?; two features that correlate differently with Treatment and similarly with oucome)?)\n",
        "#    4) Explain how why your model is better for causal estimation (e.g. bias v variance)\n",
        "#    5) Present the coefficient size of the variables\n",
        "#    6) Advise the govt which interventions will reduce crime\n",
        "#    7) Talk about potential problems with your conclusions/ recommendations (e.g. bias from omitted variables)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c9ac8a27",
      "metadata": {
        "id": "c9ac8a27"
      },
      "outputs": [],
      "source": [
        "# Group 1: Run an OLS\n",
        "\n",
        "# you will use a theory-based model (make up your own theory about \n",
        "# which are the important features - go into variable list and pick features)\n",
        "# please pick roughly 20 features....I have picked 11 for you below\n",
        "\n",
        "# step 1: create dataframes\n",
        "y_ols = df['ViolentCrimesPerPop'].copy()\n",
        "X_ols = df[['PolicBudgPerPop', 'NumStreet', 'PctNotSpeakEnglWell', 'PctFam2Par', 'PctPopUnderPov', 'perCapInc', 'agePct12t21',\n",
        "        'agePct12t29', 'agePct16t24', 'agePct65up', 'LemasTotReqPerPop']].copy()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "98ad1716",
      "metadata": {
        "id": "98ad1716"
      },
      "outputs": [],
      "source": [
        "# Group 1: Run an OLS\n",
        "# step 2: look at your dataframes: know which features you must clean\n",
        "X_ols.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ae72e56c",
      "metadata": {
        "id": "ae72e56c"
      },
      "outputs": [],
      "source": [
        "# Group 1: Run an OLS\n",
        "# step 3: Impute median values of missing X variables\n",
        "# We use a pipeline because, later, when we do ML you need to partition \n",
        "# your data and you may only want to do things in one partition of the data \n",
        "# and not the other\n",
        "\n",
        "# Let's create a ML pipeline where we fill these with median values\n",
        "# NOTE: We'll also add a data standardization step to the pipeline\n",
        "import numpy as np\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "# Ordinary Least Square - all variables using sklearn\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "ols = LinearRegression()\n",
        "\n",
        "pipe = Pipeline([\n",
        "    ('imputer', SimpleImputer(missing_values=np.nan, strategy='median')),\n",
        "    ('standardise', StandardScaler()),\n",
        "    ('estimator', LinearRegression())\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "79fe783d",
      "metadata": {
        "id": "79fe783d"
      },
      "outputs": [],
      "source": [
        "# Group 1: Run an OLS\n",
        "# step 4: run your regression\n",
        "# Now do the fit on the whole dataset, and check the R-squared\n",
        "pipe.fit(X=X_ols, y=y_ols)\n",
        "# and check the goodness of the fit\n",
        "# Note: this R-sq the in-sample R-sq\n",
        "print('R-squared for OLS fit:')\n",
        "pipe.score(X=X_ols, y=y_ols)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "81b4315c",
      "metadata": {
        "id": "81b4315c"
      },
      "outputs": [],
      "source": [
        "# Group 1: Run an OLS\n",
        "\n",
        "# step 5: check the goodness of the fit out-of-sample\n",
        "\n",
        "# Let's get an idea of how generalisable the OLS regression is\n",
        "# Using train/test splitting\n",
        "# Note: training OLS on 2/3 of the 'dropmiss' data then testing on 1/3 of the 'dropmiss' data\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "X_ols_train, X_ols_test, y_ols_train, y_ols_test = train_test_split(X_ols, y_ols, test_size=0.33, random_state=42)\n",
        "pipe.fit(X=X_ols_train, y=y_ols_train)\n",
        "R_squared_train = pipe.score(X=X_ols_train, y=y_ols_train)\n",
        "R_squared_test = pipe.score(X=X_ols_test, y=y_ols_test)\n",
        "\n",
        "plt.bar(x=[0,1], height=[R_squared_train, R_squared_test], tick_label=['Train', 'Test'])\n",
        "plt.ylabel(r'$R^2$')\n",
        "plt.title('test vs train scores')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "print('R-squared for OLS fit - out-of-sample:')\n",
        "pipe.score(X=X_ols_test, y=y_ols_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f51ce758",
      "metadata": {
        "id": "f51ce758"
      },
      "outputs": [],
      "source": [
        "# Group 1: Run an OLS\n",
        "\n",
        "# step 6: represent your results in a table and graph\n",
        "# Show the feature importance (absolute coefficient values) for each independent variable\n",
        "estimator = pipe.named_steps.estimator\n",
        "\n",
        "coef_df = pd.DataFrame(data=[estimator.coef_], columns=pipe.named_steps.imputer.feature_names_in_, index=['OLS'])\n",
        "coef_df \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aeb1eb11",
      "metadata": {
        "id": "aeb1eb11"
      },
      "outputs": [],
      "source": [
        "# Group 1: Run an OLS\n",
        "# step 6: represent your results in a table and graph\n",
        "\n",
        "# And graphically\n",
        "fig, ax = plt.subplots()\n",
        "barh = plt.barh(y=range(len(X_ols.columns)), width=estimator.coef_)\n",
        "ax.set_yticks(ticks=range(len(X_ols.columns)), labels=pipe.named_steps.imputer.feature_names_in_)\n",
        "ax.invert_yaxis()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a71b7290",
      "metadata": {
        "id": "a71b7290"
      },
      "outputs": [],
      "source": [
        "# Group 2: Run a LASSO\n",
        "\n",
        "# Step 1: create dataframes\n",
        "y_ml_lasso = df['ViolentCrimesPerPop'].copy()\n",
        "X_ml_lasso = df.drop(\n",
        "    columns=['state', 'county', 'community', 'communityname', 'fold', 'ViolentCrimesPerPop']\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "701f3c8e",
      "metadata": {
        "id": "701f3c8e"
      },
      "outputs": [],
      "source": [
        "# Group 2: Run a LASSO\n",
        "# step 2: look at your dataframes: know which features you must clean\n",
        "X_ml_lasso.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "28a9e010",
      "metadata": {
        "id": "28a9e010"
      },
      "outputs": [],
      "source": [
        "# Group 2: Run a LASSO\n",
        "\n",
        "# step 3: Impute median values of missing X variables, standardise and set lasso\n",
        "# We use a pipeline because, later, when we do ML you need to partition \n",
        "# your data and you may only want to do things in one partition of the data \n",
        "# and not the other\n",
        "\n",
        "# Let's create a ML pipeline where we fill these with median values\n",
        "# NOTE: We'll also add a data standardization step to the pipeline\n",
        "import numpy as np\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.linear_model import Lasso\n",
        "\n",
        "pipe = Pipeline([\n",
        "    ('imputer', SimpleImputer(missing_values=np.nan, strategy='median')),\n",
        "    ('standardise', StandardScaler()),\n",
        "    ('estimator', Lasso())\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e9b6d7d3",
      "metadata": {
        "id": "e9b6d7d3"
      },
      "outputs": [],
      "source": [
        "# Group 2: Run a LASSO\n",
        "\n",
        "# step 4: set your gridsearchCV values\n",
        "\n",
        "grid = {\n",
        "    'estimator__alpha': np.logspace(-3, 1, 100)\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "90f8f233",
      "metadata": {
        "id": "90f8f233"
      },
      "outputs": [],
      "source": [
        "# Group 2: Run a LASSO\n",
        "\n",
        "# step 5: Do CV: one outer fold (1/3 test; 2/3 train) and 3 inner folds\n",
        "\n",
        "# For all the ML models, we're going to use GridSearchCV to optimise the hyperparameters\n",
        "# Problem: How to ensure that the models produced in this way are generalisable/minimse overfitting?\n",
        "# Answer: We'll split the whole dataset into training data and test data, run the GridSearchCV on the training data, \n",
        "# then look at the test vs train score\n",
        "# If we're happy with this, then we can proceed to fitting the full dataset\n",
        "\n",
        "X_lasso_train, X_lasso_test, y_lasso_train, y_lasso_test = train_test_split(X_ml_lasso, y_ml_lasso, test_size=0.33, random_state=42)\n",
        "grid_search_cv = GridSearchCV(estimator=pipe, param_grid=grid, cv=3)\n",
        "grid_search_cv.fit(X=X_lasso_train, y=y_lasso_train)\n",
        "R_squared_train = grid_search_cv.score(X=X_lasso_train, y=y_lasso_train)\n",
        "R_squared_test = grid_search_cv.score(X=X_lasso_test, y=y_lasso_test)\n",
        "\n",
        "plt.bar(x=[0,1], height=[R_squared_train, R_squared_test], tick_label=['Train', 'Test'])\n",
        "plt.title('Lasso Test Score vs Train Score')\n",
        "plt.show()\n",
        "\n",
        "# Note if you wanted to do other types of CV such as nested or LOO\n",
        "# see Week 5 code\n",
        "# but essentially something like (instead of above code) for nestedCV\n",
        "\n",
        "#....\n",
        "# import matplotlib.pyplot as plt\n",
        "# from sklearn.model_selection import cross_validate \n",
        "# scoring = ['r2', 'neg_mean_squared_error']\n",
        "\n",
        "# grid_search_cv = GridSearchCV(estimator=pipe, param_grid=grid, cv=3, return_train_score=True, scoring=scoring, \n",
        "#                               refit='neg_mean_squared_error')\n",
        "# cv_result_cv = cross_validate(estimator=grid_search_cv, X=X, y=y, cv=3, return_train_score=True, return_estimator=True,\n",
        "#                            scoring=scoring)\n",
        "\n",
        "# # R^2\n",
        "# # Note: these are the average scores across the 3 outer folds..using the 'best' lambda from each outer fold\n",
        "\n",
        "# test_score = cv_result_cv['test_r2']\n",
        "# train_score = cv_result_cv['train_r2']\n",
        "# plt.bar(x=range(2), \n",
        "#         height = [np.mean(train_score), np.mean(test_score)], \n",
        "#         tick_label=['Train', 'Test'], \n",
        "#         yerr=[np.std(train_score), np.std(test_score)])\n",
        "# plt.ylabel(r'$R^2$')\n",
        "# plt.title('\"Outer\" cross-validation scores ($R^2$)')\n",
        "# plt.show()\n",
        "\n",
        "# # MSE\n",
        "# # Note: these are the average scores across the 3 outer folds..using the 'best'  lambda from each outer fold\n",
        "\n",
        "# test_score = cv_result_cv['test_neg_mean_squared_error']\n",
        "# train_score = cv_result_cv['train_neg_mean_squared_error']\n",
        "# plt.bar(x=range(2), \n",
        "#         height = np.multiply(-1, [np.mean(train_score), np.mean(test_score)]), \n",
        "#         tick_label=['Train', 'Test'], \n",
        "#         yerr=[np.std(train_score), np.std(test_score)])\n",
        "# plt.ylabel(r'MSE')\n",
        "# plt.title('\"Outer\" cross-validation scores (MSE)')\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "edfc4be5",
      "metadata": {
        "id": "edfc4be5"
      },
      "outputs": [],
      "source": [
        "# Group 2: Run a LASSO\n",
        "\n",
        "# step 6: Fit the whole dataset\n",
        "# Note: this re-runs the entire pipeline on the full data set.\n",
        "\n",
        "# The idea is that the cross-validation step shows that\n",
        "# the results from the pipeline in question are generalisable, \n",
        "# after which you can repeat the pipeline on the full data set\n",
        "\n",
        "grid_search_cv = GridSearchCV(estimator=pipe, param_grid=grid, cv=3)\n",
        "_ = grid_search_cv.fit(X=X_ml_lasso, y=y_ml_lasso)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0abf3813",
      "metadata": {
        "id": "0abf3813"
      },
      "outputs": [],
      "source": [
        "# Group 2: Run a LASSO\n",
        "\n",
        "# step 7: look at the coefficients chosen\n",
        "non_zero_coef = grid_search_cv.best_estimator_.named_steps.estimator.coef_.nonzero()\n",
        "\n",
        "non_zero_coef_df = pd.DataFrame(data=[grid_search_cv.best_estimator_.named_steps.estimator.coef_[non_zero_coef]],\n",
        "                                columns=grid_search_cv.best_estimator_.named_steps.imputer.feature_names_in_[non_zero_coef], index=['Lasso'])\n",
        "display(non_zero_coef_df)\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "barh = plt.barh(y=range(len(X_ml_lasso.columns[non_zero_coef])), \n",
        "                width=grid_search_cv.best_estimator_.named_steps.estimator.coef_[non_zero_coef])\n",
        "ax.set_yticks(ticks=range(len(X_ml_lasso.columns[non_zero_coef])), \n",
        "              labels=grid_search_cv.best_estimator_.named_steps.imputer.feature_names_in_[non_zero_coef])\n",
        "ax.invert_yaxis()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9928cdbe",
      "metadata": {
        "id": "9928cdbe"
      },
      "outputs": [],
      "source": [
        "# save your coefficients\n",
        "coef_df = pd.concat([coef_df, \n",
        "                     pd.DataFrame(data=[grid_search_cv.best_estimator_.named_steps.estimator.coef_],\n",
        "                                  columns=grid_search_cv.best_estimator_.named_steps.imputer.feature_names_in_,\n",
        "                                  index=['Lasso',])]).fillna(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8ffd7336",
      "metadata": {
        "id": "8ffd7336"
      },
      "outputs": [],
      "source": [
        "# Step 8: traditionally for causal estimation, we can do a post-ols\n",
        "# for this exercise, we will simplify and skip this step"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "20e3c879",
      "metadata": {
        "id": "20e3c879"
      },
      "outputs": [],
      "source": [
        "# Compare the coefficients produced by OLS vs Lasso\n",
        "non_zero_coef = (coef_df != 0).any()\n",
        "ols_lasso_coef_compare_df = coef_df[non_zero_coef[non_zero_coef].index.to_list()].copy()\n",
        "\n",
        "ind = np.arange(len(ols_lasso_coef_compare_df.columns))\n",
        "width = 0.4\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "fig.set_size_inches(10, 16)\n",
        "ax.barh(ind, ols_lasso_coef_compare_df.loc['OLS'], width, label='OLS')\n",
        "ax.barh(ind + width, ols_lasso_coef_compare_df.loc['Lasso'], width, label='Lasso')\n",
        "\n",
        "ax.invert_yaxis()\n",
        "ax.set_yticks(ticks=ind + width, labels=ols_lasso_coef_compare_df.columns)\n",
        "ax.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}