{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: Although this exercise seeks to replicate the ML process 'in real life', in some places this is not possible\n",
    "# for example, where 'realistic' estimator fits/grid searches might take a significant amount of time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import  pandas as pd\n",
    "\n",
    "# read in the file - which in this case is in csv - comma separated value - format\n",
    "# we use:\n",
    "#   sep = ',' to set the separator (noting that comma is actually the default so this is not necessary to include)\n",
    "#   header = None as the dataset does not have a header row\n",
    "#   na_values = '?' as this dataset uses the ? to represent missing values\n",
    "# NOTES:\n",
    "#   - if a separator other than ',' was used we would use the 'sep=' parameter\n",
    "#   - if the data were in Excel format we can use pd.read_excel - and there are a number of other methods for different datatypes\n",
    "df = pd.read_csv(filepath_or_buffer='./communities.data', sep=',', header=None, na_values='?', keep_default_na=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check - we should have 1994 rows and 128 columns\n",
    "print(f'There are {len(df)} rows')\n",
    "print(f'and {len(df.columns)} columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The data file didn't include a header row, so we'll set the column names now\n",
    "# NOTE: We could also have done this as part of the read_csv call\n",
    "column_names = ['state', 'county', 'community', 'communityname', 'fold', 'population', 'householdsize', 'racepctblack', 'racePctWhite', 'racePctAsian', 'racePctHisp', 'agePct12t21', 'agePct12t29', 'agePct16t24', 'agePct65up', 'numbUrban', 'pctUrban', 'medIncome', 'pctWWage', 'pctWFarmSelf', 'pctWInvInc', 'pctWSocSec', 'pctWPubAsst', 'pctWRetire', 'medFamInc', 'perCapInc', 'whitePerCap', 'blackPerCap', 'indianPerCap', 'AsianPerCap', 'OtherPerCap', 'HispPerCap', 'NumUnderPov', 'PctPopUnderPov', 'PctLess9thGrade', 'PctNotHSGrad', 'PctBSorMore', 'PctUnemployed', 'PctEmploy', 'PctEmplManu', 'PctEmplProfServ', 'PctOccupManu', 'PctOccupMgmtProf', 'MalePctDivorce', 'MalePctNevMarr', 'FemalePctDiv', 'TotalPctDiv', 'PersPerFam', 'PctFam2Par', 'PctKids2Par', 'PctYoungKids2Par', 'PctTeen2Par', 'PctWorkMomYoungKids', 'PctWorkMom', 'NumIlleg', 'PctIlleg', 'NumImmig', 'PctImmigRecent', 'PctImmigRec5', 'PctImmigRec8', 'PctImmigRec10', 'PctRecentImmig', 'PctRecImmig5', 'PctRecImmig8', 'PctRecImmig10', 'PctSpeakEnglOnly', 'PctNotSpeakEnglWell', 'PctLargHouseFam', 'PctLargHouseOccup', 'PersPerOccupHous', 'PersPerOwnOccHous', 'PersPerRentOccHous', 'PctPersOwnOccup', 'PctPersDenseHous', 'PctHousLess3BR', 'MedNumBR', 'HousVacant', 'PctHousOccup', 'PctHousOwnOcc', 'PctVacantBoarded', 'PctVacMore6Mos', 'MedYrHousBuilt', 'PctHousNoPhone', 'PctWOFullPlumb', 'OwnOccLowQuart', 'OwnOccMedVal', 'OwnOccHiQuart', 'RentLowQ', 'RentMedian', 'RentHighQ', 'MedRent', 'MedRentPctHousInc', 'MedOwnCostPctInc', 'MedOwnCostPctIncNoMtg', 'NumInShelters', 'NumStreet', 'PctForeignBorn', 'PctBornSameState', 'PctSameHouse85', 'PctSameCity85', 'PctSameState85', 'LemasSwornFT', 'LemasSwFTPerPop', 'LemasSwFTFieldOps', 'LemasSwFTFieldPerPop', 'LemasTotalReq', 'LemasTotReqPerPop', 'PolicReqPerOffic', 'PolicPerPop', 'RacialMatchCommPol', 'PctPolicWhite', 'PctPolicBlack', 'PctPolicHisp', 'PctPolicAsian', 'PctPolicMinor', 'OfficAssgnDrugUnits', 'NumKindsDrugsSeiz', 'PolicAveOTWorked', 'LandArea', 'PopDens', 'PctUsePubTrans', 'PolicCars', 'PolicOperBudg', 'LemasPctPolicOnPatr', 'LemasGangUnitDeploy', 'LemasPctOfficDrugUn', 'PolicBudgPerPop', 'ViolentCrimesPerPop']\n",
    "df.columns = column_names\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we'll take a quick look at the variables\n",
    "# we'll use include='all' to show all columns, not just numeric ones\n",
    "df.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Depending on your notebook settings this might not show all columns - if it isn't, we can change this\n",
    "pd.options.display.max_columns = None\n",
    "df.describe(include='all')\n",
    "\n",
    "# NOTE: There are a number of variables/columns with incomplete data (count < 1994)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at our dependent variable - ViolentCrimesPerPop\n",
    "df['ViolentCrimesPerPop'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's show a histogram of ViolentCrimesPerPop for the first 10 communities\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.bar(x=range(len(df[:10])), height=df['ViolentCrimesPerPop'][:10], tick_label=df['communityname'][:10])\n",
    "plt.ylabel('ViolentCrimesPerPop')\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ViolentCrimesPerPop vs PctFam2Par\n",
    "plt.scatter(x=df['ViolentCrimesPerPop'], y=df['PctFam2Par'])\n",
    "plt.ylabel('ViolentCrimesPerPop')\n",
    "plt.xlabel('PctFam2Par')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now do an OLS\n",
    "# Our dependent variable is: ViolentCrimesPerPop: total number of violent crimes per 100K popuation (numeric - decimal) GOAL attribute (to be predicted)\n",
    "\n",
    "# Our independent variables are:\n",
    "#   PolicBudgPerPop: police operating budget per population (numeric - decimal)\n",
    "#   NumStreet: number of homeless people counted in the street (numeric - decimal)\n",
    "#   PctNotSpeakEnglWell: percent of people who do not speak English well (numeric - decimal)\n",
    "#   PctFam2Par: percentage of families (with kids) that are headed by two parents (numeric - decimal)\n",
    "#   PctPopUnderPov: percentage of people under the poverty level (numeric - decimal)\n",
    "#   perCapInc: per capita income (numeric - decimal)\n",
    "#   agePct12t21: percentage of population that is 12-21 in age (numeric - decimal)\n",
    "#   agePct12t29: percentage of population that is 12-29 in age (numeric - decimal)\n",
    "#   agePct16t24: percentage of population that is 16-24 in age (numeric - decimal)\n",
    "#   agePct65up: percentage of population that is 65 and over in age (numeric - decimal)\n",
    "#   LemasTotReqPerPop: total requests for police per 100K popuation (numeric - decimal)\n",
    "\n",
    "y_ols = df['ViolentCrimesPerPop'].copy()\n",
    "X_ols = df[['PolicBudgPerPop', 'NumStreet', 'PctNotSpeakEnglWell', 'PctFam2Par', 'PctPopUnderPov', 'perCapInc', 'agePct12t21',\n",
    "        'agePct12t29', 'agePct16t24', 'agePct65up', 'LemasTotReqPerPop']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ols.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ordinary Least Square - all variables using sklearn\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "ols = LinearRegression()\n",
    "ols.fit(X=X_ols, y=y_ols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Oops! We should have noticed that PolicBudgPerPop and LemasTotReqPerPop are only valid for a subset of the data!\n",
    "# Only 319 rows have valid values for these - \n",
    "\n",
    "\n",
    "# one thing we can do with missing is to drop all missing\n",
    "\n",
    "# drop na rows from the variables we're actually using\n",
    "\n",
    "# drop missing data if necessary i.e. \n",
    "X_ols_dropmiss = X_ols.dropna().copy()\n",
    "y_ols_dropmiss = y_ols[X_ols_dropmiss.index].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run the OLS on the rows where there is no-missing\n",
    "\n",
    "ols = LinearRegression()\n",
    "ols.fit(X=X_ols_dropmiss, y=y_ols_dropmiss)\n",
    "# and check the goodness of the fit\n",
    "print('R-squared for OLS fit:')\n",
    "ols.score(X=X_ols_dropmiss, y=y_ols_dropmiss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the feature importance (absolute coefficient values) for each independent variable\n",
    "# print out the results from the ols regression above \n",
    "\n",
    "coef_df_dropmiss = pd.DataFrame(data=[ols.coef_], columns=X_ols_dropmiss.columns, index=['OLS'])\n",
    "coef_df_dropmiss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#but then you're only left with 319 observations where there's full info in every row!\n",
    "#why waste data like that?\n",
    "X_ols_dropmiss.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute median values of missing X variables\n",
    "# We use a pipeline because, later, when we do ML you need to partition \n",
    "# your data and you may only want to do things in one partition of the data \n",
    "# and not the other\n",
    "\n",
    "# Let's create a ML pipeline where we fill these with median values\n",
    "# NOTE: We'll also add a data standardization step to the pipeline\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('imputer', SimpleImputer(missing_values=np.nan, strategy='median')),\n",
    "    ('standardise', StandardScaler()),\n",
    "    ('estimator', LinearRegression())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now do the fit on the whole dataset, and check the R-squared\n",
    "pipe.fit(X=X_ols, y=y_ols)\n",
    "\n",
    "# and check the goodness of the fit\n",
    "print('R-squared for OLS fit:')\n",
    "pipe.score(X=X_ols, y=y_ols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the feature importance (absolute coefficient values) for each independent variable\n",
    "estimator = pipe.named_steps.estimator\n",
    "\n",
    "coef_df = pd.DataFrame(data=[estimator.coef_], columns=pipe.named_steps.imputer.feature_names_in_, index=['OLS'])\n",
    "coef_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And graphically\n",
    "fig, ax = plt.subplots()\n",
    "barh = plt.barh(y=range(len(X_ols.columns)), width=estimator.coef_)\n",
    "ax.set_yticks(ticks=range(len(X_ols.columns)), labels=pipe.named_steps.imputer.feature_names_in_)\n",
    "ax.invert_yaxis()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's do some machine learning!\n",
    "# For the machine learning models we're going to use a much bigger dataset of variables\n",
    "# Note: we have created a new df for the ML runs\n",
    "# Note: it's usually ok to reuse the same dfs for ols and ml, \n",
    "# but here we'll use different df names to make it clear\n",
    "\n",
    "y_ml = df['ViolentCrimesPerPop'].copy()\n",
    "X_ml = df.drop(\n",
    "    columns=['state', 'county', 'community', 'communityname', 'fold', 'ViolentCrimesPerPop']\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For all the ML models, we're going to use GridSearchCV to optimise the hyperparameters\n",
    "# Problem: How to ensure that the models produced in this way are generalisable/minimse overfitting?\n",
    "# Answer: We'll split the whole dataset into training data and test data, run the GridSearchCV on the training data, \n",
    "# then look at the test vs train score\n",
    "# If we're happy with this, then we can proceed to fitting the full dataset\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "grid = {\n",
    "    'estimator__alpha': np.logspace(-3, 1, 100)\n",
    "}\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('imputer', SimpleImputer(missing_values=np.nan, strategy='median')),\n",
    "    ('standardise', StandardScaler()),\n",
    "    ('estimator', Lasso())\n",
    "])\n",
    "\n",
    "X_lasso_train, X_lasso_test, y_lasso_train, y_lasso_test = train_test_split(X_ml, y_ml, test_size=0.33, random_state=42)\n",
    "grid_search_cv = GridSearchCV(estimator=pipe, param_grid=grid, cv=3)\n",
    "grid_search_cv.fit(X=X_lasso_train, y=y_lasso_train)\n",
    "R_squared_train = grid_search_cv.score(X=X_lasso_train, y=y_lasso_train)\n",
    "R_squared_test = grid_search_cv.score(X=X_lasso_test, y=y_lasso_test)\n",
    "\n",
    "plt.bar(x=[0,1], height=[R_squared_train, R_squared_test], tick_label=['Train', 'Test'])\n",
    "plt.title('Lasso Test Score vs Train Score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the whole dataset\n",
    "grid_search_cv = GridSearchCV(estimator=pipe, param_grid=grid, cv=3)\n",
    "_ = grid_search_cv.fit(X=X_ml, y=y_ml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And look at the coefficients chosen\n",
    "fig, ax = plt.subplots()\n",
    "barh = plt.barh(y=range(len(X_ml.columns)), width=grid_search_cv.best_estimator_.named_steps.estimator.coef_)\n",
    "ax.set_yticks(ticks=range(len(X_ml.columns)), labels=grid_search_cv.best_estimator_.named_steps.imputer.feature_names_in_)\n",
    "ax.invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# That's not super-useful - let's just look at the ones that are non-zero\n",
    "non_zero_coef = grid_search_cv.best_estimator_.named_steps.estimator.coef_.nonzero()\n",
    "\n",
    "non_zero_coef_df = pd.DataFrame(data=[grid_search_cv.best_estimator_.named_steps.estimator.coef_[non_zero_coef]],\n",
    "                                columns=grid_search_cv.best_estimator_.named_steps.imputer.feature_names_in_[non_zero_coef], index=['Lasso'])\n",
    "display(non_zero_coef_df)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "barh = plt.barh(y=range(len(X_ml.columns[non_zero_coef])), \n",
    "                width=grid_search_cv.best_estimator_.named_steps.estimator.coef_[non_zero_coef])\n",
    "ax.set_yticks(ticks=range(len(X_ml.columns[non_zero_coef])), \n",
    "              labels=grid_search_cv.best_estimator_.named_steps.imputer.feature_names_in_[non_zero_coef])\n",
    "ax.invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coef_df = pd.concat([coef_df, \n",
    "                     pd.DataFrame(data=[grid_search_cv.best_estimator_.named_steps.estimator.coef_],\n",
    "                                  columns=grid_search_cv.best_estimator_.named_steps.imputer.feature_names_in_,\n",
    "                                  index=['Lasso',])]).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the coefficients produced by OLS vs Lasso\n",
    "non_zero_coef = (coef_df != 0).any()\n",
    "ols_lasso_coef_compare_df = coef_df[non_zero_coef[non_zero_coef].index.to_list()].copy()\n",
    "\n",
    "ind = np.arange(len(ols_lasso_coef_compare_df.columns))\n",
    "width = 0.4\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(10, 16)\n",
    "ax.barh(ind, ols_lasso_coef_compare_df.loc['OLS'], width, label='OLS')\n",
    "ax.barh(ind + width, ols_lasso_coef_compare_df.loc['Lasso'], width, label='Lasso')\n",
    "\n",
    "ax.invert_yaxis()\n",
    "ax.set_yticks(ticks=ind + width, labels=ols_lasso_coef_compare_df.columns)\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the independent variable that was strongest in OLS was PctFam2Par\n",
    "# It was not even picked in LASSO - why do you think that is?\n",
    "# Note: LASSO picked PctKids2Par\n",
    "\n",
    "# Let's look at the correlation between PctFam2Par and other variables picked by OLS or Lasso\n",
    "pctfam2par_correlation = X_ml[ols_lasso_coef_compare_df.columns.to_list()].corr()['PctFam2Par']\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(10, 10)\n",
    "plt.barh(y=range(len(pctfam2par_correlation.index)), width=pctfam2par_correlation)\n",
    "ax.invert_yaxis()\n",
    "ax.set_yticks(ticks=range(len(pctfam2par_correlation.index)), labels=ols_lasso_coef_compare_df.columns)\n",
    "plt.tight_layout()\n",
    "plt.title('Correlation with PctFam2Par')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OK, we've compared two models: OLS and LASSO\n",
    "# which one should we rely on?\n",
    "# depends on your aim: causal estimation or prediction\n",
    "# say we focus on prediction for now\n",
    "# one way to judge how 'predictive' a model is,\n",
    "# is to look at how it performs out of sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's get an idea of how generalisable the OLS regression is\n",
    "# Using train/test splitting\n",
    "# Note: training OLS on 2/3 of the 'dropmiss' data then testing on 1/3 of the 'dropmiss' data\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_ols_train, X_ols_test, y_ols_train, y_ols_test = train_test_split(X_ols_dropmiss, y_ols_dropmiss, test_size=0.33, random_state=42)\n",
    "ols.fit(X=X_ols_train, y=y_ols_train)\n",
    "R_squared_train = ols.score(X=X_ols_train, y=y_ols_train)\n",
    "R_squared_test = ols.score(X=X_ols_test, y=y_ols_test)\n",
    "\n",
    "plt.bar(x=[0,1], height=[R_squared_train, R_squared_test], tick_label=['Train', 'Test'])\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now do the fit on the whole dataset, and check the R-squared\n",
    "# Note: this R-sq the in-sample R-sq\n",
    "\n",
    "ols.fit(X=X_ols_dropmiss, y=y_ols_dropmiss)\n",
    "\n",
    "\n",
    "# and check the goodness of the fit\n",
    "print('R-squared for OLS fit:')\n",
    "ols.score(X=X_ols_dropmiss, y=y_ols_dropmiss)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For all the ML models, we're going to use GridSearchCV to optimise the hyperparameters\n",
    "# Problem: How to ensure that the models produced in this way are generalisable/minimse overfitting?\n",
    "# Answer: We'll use Nested CV - a more powerful version of the train/test splitting we did above, to look at the the test vs train score\n",
    "# If we're happy with this, then we can proceed to fitting the full dataset\n",
    "\n",
    "from sklearn.model_selection import cross_validate, GridSearchCV\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "grid = {\n",
    "    'estimator__alpha': np.logspace(-3, 1, 100)\n",
    "}\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('imputer', SimpleImputer(missing_values=np.nan, strategy='median')),\n",
    "    ('standardise', StandardScaler()),\n",
    "    ('estimator', Lasso())\n",
    "])\n",
    "\n",
    "cv_result = cross_validate(GridSearchCV(estimator=pipe, param_grid=grid, cv=3), X=X_ml, y=y_ml, cv=3, return_train_score=True)\n",
    "\n",
    "# Note: by doing the gridsearchCV within the cross-validate function –  this is how we’re doing the nestedCV\n",
    "# The 'cross_validate' function splits the data in test (1/3) and train (2/3), \n",
    "# then passes the training set on to GridSearchCV to do it's own hyperparmater optimisation (using CV). \n",
    "# This is repeated 3 times.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_score = cv_result['test_score']\n",
    "train_score = cv_result['train_score']\n",
    "plt.bar(x=range(2), height = [np.mean(train_score), np.mean(test_score)], tick_label=['Train', 'Test'], \n",
    "        yerr=[np.std(train_score), np.std(test_score)])\n",
    "plt.title('LASSO CV')\n",
    "plt.show()\n",
    "\n",
    "# Note: the cv_results in:\n",
    "# test_score = cv_result['test_score'] is based on the 33% of full non-missing data or the outer holdout\n",
    "# i.e. the R-squared using the best model from the GridSearchCV on the data initially held out (3 values)\n",
    "# train_score = cv_result['train_score'] is based on the 66% of the full non-missing data\n",
    "# i.e. the R-squared from the best model from the GridSearchCV but on the data used by the GridSearchCV\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the whole dataset\n",
    "grid_search_cv = GridSearchCV(estimator=pipe, param_grid=grid, cv=3)\n",
    "\n",
    "\n",
    "# Note: in line GridSearchCV(estimator=pipe, param_grid=grid, cv=3\n",
    "# the GridSearchCV only sees 2/3 of the data (each iteration), and further divides this into 1/3 and 2/3\n",
    "# for hyperparameter optimisation\n",
    "\n",
    "_ = grid_search_cv.fit(X=X_ml, y=y_ml)\n",
    "# this re-runs the entire pipeline on the full data set. \n",
    "# The idea is that the cross-validation step shows that\n",
    "# the results from the pipeline in question are generalisable, \n",
    "# after which you can repeat the pipeline on the full data set\n",
    "# so the hyperparameters chosen in the nestedCV are likely to differ from \n",
    "# the hyperparams chosen in this pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now repeat something similar for Ridge\n",
    "# NOTE: This time we'll use the cross-validate function, which essentially performs nested cross validation \n",
    "# - performing multiple train/test splits\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "grid = {\n",
    "    'estimator__alpha': np.logspace(-3, 2, 100)\n",
    "}\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('imputer', SimpleImputer(missing_values=np.nan, strategy='median')),\n",
    "    ('standardise', StandardScaler()),\n",
    "    ('estimator', Ridge())\n",
    "])\n",
    "\n",
    "grid_search_cv = GridSearchCV(estimator=pipe, param_grid=grid, cv=3)\n",
    "cv_result = cross_validate(grid_search_cv, X=X_ml, y=y_ml, cv=3, return_train_score=True)\n",
    "test_score = cv_result['test_score']\n",
    "train_score = cv_result['train_score']\n",
    "plt.bar(x=range(2), height = [np.mean(train_score), np.mean(test_score)], tick_label=['Train', 'Test'], \n",
    "        yerr=[np.std(train_score), np.std(test_score)])\n",
    "plt.title('Ridge CV')\n",
    "plt.show()\n",
    "\n",
    "grid_search_cv.fit(X=X_ml, y=y_ml)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(10, 24)\n",
    "plt.barh(y=range(len(X_ml.columns)), \n",
    "         width=grid_search_cv.best_estimator_.named_steps.estimator.coef_)\n",
    "ax.set_yticks(ticks=range(len(X_ml.columns)),\n",
    "               labels=grid_search_cv.best_estimator_.named_steps.imputer.feature_names_in_)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coef_df = pd.concat([coef_df,\n",
    "                     pd.DataFrame(data=[grid_search_cv.best_estimator_.named_steps.estimator.coef_],\n",
    "                                  columns=grid_search_cv.best_estimator_.named_steps.imputer.feature_names_in_,\n",
    "                                  index=['Ridge',])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And now GBR\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "grid = {\n",
    "    # 'estimator__learning_rate': np.logspace(-2, 1, 3),\n",
    "    'estimator__learning_rate': np.logspace(-2, -0.5, 3),\n",
    "    'estimator__min_samples_split': np.logspace(-2, 0, 3),\n",
    "    'estimator__max_depth': np.arange(1, 5, 2)\n",
    "}\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('imputer', SimpleImputer()),\n",
    "    ('standardise', StandardScaler()),\n",
    "    ('estimator', GradientBoostingRegressor(random_state=42))\n",
    "])\n",
    "\n",
    "grid_search_cv = GridSearchCV(estimator=pipe, param_grid=grid, cv=3)\n",
    "cv_result = cross_validate(estimator=grid_search_cv, X=X_ml, y=y_ml, cv=3, return_train_score=True)\n",
    "test_score = cv_result['test_score']\n",
    "train_score = cv_result['train_score']\n",
    "plt.bar(x=range(2), \n",
    "        height = [np.mean(train_score), np.mean(test_score)], \n",
    "        tick_label=['Train', 'Test'], \n",
    "        yerr=[np.std(train_score), np.std(test_score)])\n",
    "plt.title('GBR CV')\n",
    "plt.show()\n",
    "\n",
    "grid_search_cv.fit(X=X_ml, y=y_ml)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(10,24)\n",
    "plt.barh(y=range(len(X_ml.columns)), \n",
    "         width=np.abs(grid_search_cv.best_estimator_.named_steps.estimator.feature_importances_))\n",
    "ax.set_yticks(ticks=range(len(X_ml.columns)),\n",
    "              labels=grid_search_cv.best_estimator_.named_steps.imputer.feature_names_in_)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coef_df = pd.concat([coef_df,\n",
    "                     pd.DataFrame(data=[grid_search_cv.best_estimator_.named_steps.estimator.feature_importances_],\n",
    "                                  columns=grid_search_cv.best_estimator_.named_steps.imputer.feature_names_in_,\n",
    "                                  index=['GBR',])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coef_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the coefficients produced by OLS vs Lasso vs Ridge vs GBR\n",
    "# NOTE: GBR doesn't produce coefficients, but does have the concept of feature importance\n",
    "# we'll need to take the absolute coefficient value to compare though\n",
    "non_zero_coef = (coef_df != 0).any()\n",
    "all_coef_compare_df = coef_df[non_zero_coef[non_zero_coef].index.to_list()].copy()\n",
    "\n",
    "ind = np.arange(len(all_coef_compare_df.columns))\n",
    "width = 0.2\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(10, 24)\n",
    "ax.barh(ind, np.abs(all_coef_compare_df.loc['OLS']), width, label='OLS')\n",
    "ax.barh(ind + width, np.abs(all_coef_compare_df.loc['Lasso']), width, label='Lasso')\n",
    "ax.barh(ind + (2 * width), np.abs(all_coef_compare_df.loc['Ridge']), width, label='Ridge')\n",
    "ax.barh(ind + (3 * width), np.abs(all_coef_compare_df.loc['GBR']), width, label='GBR')\n",
    "\n",
    "ax.invert_yaxis()\n",
    "ax.set_yticks(ticks=ind + (2 * width), labels=all_coef_compare_df.columns)\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "255c6555a66ce6db9df18e07e27a9932955fb747c3aaf5f4a8e8bf6d74ccd056"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
