{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# ECON1611 - Assessment 2: Empirical Project in Python (Individual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Module imports\n",
    "# import modules for use below\n",
    "# (Note: Modules specific to particular machine learning examples are imported in the relevant code block)\n",
    "# 'as' allows use of an abbreviated module name\n",
    "import matplotlib.pyplot as plt     # Matplotlib for low-level plot details\n",
    "import numpy as np                  # NumPy for fast numeric operations\n",
    "import pandas as pd                 # Pandas for datasets\n",
    "import seaborn as sns               # Seaborn for easier plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set a random number generator seed to allow reproducibility\n",
    "seed = 12345"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.colab import files  # uncomment if using colab\n",
    "# uploades = files.upload()  # uncomment if using colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data into a pandas dataframe for futher analysis\n",
    "df = pd.read_csv('./bank-additional.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the first few rows to check all looks normal\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Depending on your notebook settings this might not show all columns - if it isn't, we can change this\n",
    "pd.options.display.max_columns = None\n",
    "df.describe(include='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "Often the data provided is not in the final form we want to work with. Data cleaning refers to the process whereby we transform the initial data into the final form we want/need to work with.\n",
    "The following steps do not form part of the assessment, but are a demonstration of some of the steps which may be involved in data cleaning.\n",
    "In the following example this process includes:\n",
    "- Removing unneeded data\n",
    "- Transforming data - in this case, transforming categorical data into binary data (one hot encoding) \n",
    "\n",
    "**Notes:**\n",
    "\n",
    "_-When working with Pandas axis=0 means a row operation and axis=1 means a column operation._\n",
    "\n",
    "_-The a subset of columns can be selected by creating a list of names e.g. ['var1', 'var2']._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove unneeded data\n",
    "Unneeded data unnecessarily complicates the machine learning pipeline. Here we drop entire columns which are unneeded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note - `inplace=True` affects the current dataframe directly\n",
    "#  - otherwise we would need to assign the returned dataframe to a new variable\n",
    "\n",
    "# df.drop(['contact', 'month', 'day_of_week', 'duration', 'campaign', ], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode data\n",
    "Data encoding, or transformation, involves changing the form of the data\n",
    "\n",
    "**Note:**\n",
    "\n",
    "_sklearn has it's own perprocessing classes (e.g. LabelBinarizer, OneHotEncoder) which can be used as part of a data processing pipeline to do this. For the sake of those unfamiliar with data processing in Python we have used simpler pandas methods here._\n",
    "\n",
    "#### Binary Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# `y` is currently a text column - encode all 'yes' values as `1`, everything else as `0` \n",
    "# and store in a column named 'y_encoded' (this will be our final target array)\n",
    "df['y_encoded'] = df['y'].apply(lambda row: 1 if row == 'yes' else 0)\n",
    "df.head(n=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now repeat for other columns using this encoding\n",
    "# creating dummy for default or not\n",
    "df['is_default'] = df['default'].apply(lambda row: 1 if row == 'yes' else 0)\n",
    "df['is_housing'] = df['housing'].apply(lambda row: 1 if row == 'yes' else 0)\n",
    "df['is_loan'] = df['loan'].apply(lambda row: 1 if row == 'yes' else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Treated Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a binary indicator for whether person was contacted or not in PREVIOUS marketing campaign \n",
    "# (pdays = 999 if they weren't contacted at all)\n",
    "df['treated'] = df['pdays'].apply(lambda row: 0 if row == 999 else 1)\n",
    "df[['treated', 'pdays']].head()\n",
    "df['treated'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a binary indicator for whether person was contacted or not in marketing campaign \n",
    "# (campaign = NA if they weren't contacted at all) - eveyone was contacted\n",
    "df['treated'] = df['campaign'].apply(lambda row: 0 if row <= 0 else 1)\n",
    "df[['treated', 'campaign']].head()\n",
    "df['treated'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a duration indicator \n",
    "df['treated'] = df['duration']\n",
    "df[['treated', 'duration']].head()\n",
    "\n",
    "df.head(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['treated'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note - `inplace=True` affects the current dataframe directly\n",
    "#  - otherwise we would need to assign the returned dataframe to a new variable\n",
    "\n",
    "df.drop(['contact', 'month', 'day_of_week', 'campaign', ], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Nominal data\n",
    "Nominal data is data with generally several categories, and for which there is no 'innate' ordering of the categories. This kind of data is generally encoded by creating a number of 'dummy' columns containing binary (yes/no) data - one column for each 'category' in the original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the 'marital' column is nominal data - there are several different categories, for which ordering doesn't make sense\n",
    "# we will use the pandas 'get_dummies' method to create a different binary column for each status\n",
    "# NOTE: This will create several columns named 'marital_...' for each category\n",
    "marital_dummies = pd.get_dummies(df['marital'], prefix = 'marital')\n",
    "\n",
    "# view the 'new' and original columns\n",
    "pd.concat([df['marital'], marital_dummies], axis=1).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now drop the new 'marital_unknown' variable\n",
    "marital_dummies.drop('marital_unknown', axis=1, inplace=True)\n",
    "# merge new dummies into main dataframe\n",
    "df = pd.concat([df, marital_dummies], axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# repeat for 'job'\n",
    "job_dummies = pd.get_dummies(df['job'], prefix = 'job')\n",
    "job_dummies.drop('job_unknown', axis=1, inplace=True)\n",
    "X = pd.concat([df, job_dummies], axis=1)\n",
    "\n",
    "# and 'poutcome'\n",
    "poutcome_dummies = pd.get_dummies(df['poutcome'], prefix = 'poutcome')\n",
    "df = pd.concat([df, poutcome_dummies], axis=1)\n",
    "df.head()\n",
    "\n",
    "# and 'education'\n",
    "# Note: Education is a Ordinal value (the categories have an innate order,\n",
    "# and would usually be encoded using an OrdinalEncoder \n",
    "# - for simplicity here we are again using simple binary dummy values)\n",
    "education_dummies = pd.get_dummies(df['education'], prefix = 'education')\n",
    "education_dummies.drop('education_unknown', axis=1, inplace=True)\n",
    "df = pd.concat([df, education_dummies], axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split out our X and Y data\n",
    "We split the dataset into a target array (here the column currently called `y_encoded`) and a features matrix (all other columns). By convention these are named `X` and `y`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: we use `copy` to ensure that `X` and `y` are not simply views into the existing datagrame\n",
    "y = df['y_encoded'].copy()\n",
    "X = df.copy()\n",
    "X.drop(['y_encoded', 'y'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop  the 'original' variables (now encoded as other columns)\n",
    "X.drop(['job', 'education', 'marital', 'default', 'housing', 'loan', 'pdays', 'poutcome',], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Review the final dataframe\n",
    "print(X.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Review the final target\n",
    "y.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment Questions\n",
    "### 1) Summarise and describe the data\n",
    "#### a) Print the first 20 rows of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head(n=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b) Describe the data e.g. mean, median, standard deviation of all the variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c) Count the number of observations in each response category "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of observations in each response category\n",
    "# NOTE: This is easiest done on the data before encoding\n",
    "# e.g. marital\n",
    "\n",
    "# Are there any missing values?\n",
    "print('Missing values?')\n",
    "print(df['marital'].isnull().values.any())\n",
    "\n",
    "# Count of unique values\n",
    "print('Count of unique values')\n",
    "print(df['marital'].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Graphing\n",
    "#### a) Basic scatter plot of two features against each other "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://seaborn.pydata.org/generated/seaborn.scatterplot.html\n",
    "sns.scatterplot(data=X, x=\"nr.employed\", y=\"euribor3m\")\\\n",
    "       .set(title=\"nr.employed x euribor3m\", xlabel=\"nr.employed\", ylabel=\"euribor3m\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b) Histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "sns.countplot(x=y, ax=ax)\n",
    "\n",
    "abs_vals = y.value_counts()\n",
    "rel_vals = y.value_counts(normalize=True) * 100\n",
    "labels = [f'{p[0]} ({p[1]:.0f}%)' for p in zip(abs_vals, rel_vals)]\n",
    "\n",
    "ax.bar_label(container=ax.containers[0], labels=labels)\n",
    "ax.set_title('Target distribution', fontsize=16, pad=20)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) From the 20 inputs, choose the set of controls you will use for your machine learning models. Justify why you have excluded some variables. (1 mark) \n",
    "\n",
    "**Note:**\n",
    "\n",
    "_Because we are doing this after the data cleaning step, there are now more than 20 inputs - so remember to take account variables which may now have been one-hot encoded into multiple dummy variables_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment below and choose which columns to exclude\n",
    "excluded_columns = []\n",
    "# excluded_columns = ['age', 'previous', 'emp.var.rate', 'cons.price.idx', 'cons.conf.idx',\n",
    "#        'euribor3m', 'nr.employed', 'y', 'is_default', 'is_housing', 'is_loan',\n",
    "#        'treated', 'marital_divorced', 'marital_married', 'marital_single',\n",
    "#        'poutcome_failure', 'poutcome_nonexistent', 'poutcome_success',\n",
    "#        'education_basic.4y', 'education_basic.6y', 'education_basic.9y',\n",
    "#        'education_high.school', 'education_illiterate',\n",
    "#        'education_professional.course', 'education_university.degree']\n",
    "\n",
    "X_final = X.drop(excluded_columns, axis=1)\n",
    "X_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Build a classification tree in Python\n",
    "#### a) split sample into train and test  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and Test Data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\n",
    "# Setting random_state means the split will always be the same which sometimes is useful.\n",
    "X_train, X_test, y_train, y_test =\\\n",
    "    train_test_split(X_final, y, test_size=0.3, random_state=seed)\n",
    "\n",
    "# Output the dimensions of each of the sets\n",
    "print(f\"X_train is {X_train.shape}\")\n",
    "print(f\"X_test is {X_test.shape}\")\n",
    "print(f\"y_train is {y_train.shape}\")\n",
    "print(f\"y_test is {y_test.shape}\")\n",
    "\n",
    "#why keep getting error \"inconsistent number of samples\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80",
   "metadata": {},
   "source": [
    "#### b) Without doing any pruning of the tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Setup a Decision Tree \n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html\n",
    "dt_model = DecisionTreeClassifier(criterion=\"gini\", max_depth=20, min_samples_split=2, random_state=seed)\n",
    "dt_model.fit(X_train, y_train)\n",
    "y_predict = dt_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c) Draw the tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import plot_tree\n",
    "# Plot Decision Tree using the lower-level plotting functions\n",
    "# NOTE: This may take some time to complete\n",
    "plt.figure(figsize=(12,12))\n",
    "plot_tree(dt_model, filled=True)\n",
    "plt.title(\"Unpruned Tree\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6) Calculate feature importance for each feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print 4dp of each feature importance \n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html#sklearn.tree.DecisionTreeRegressor.feature_importances_\n",
    "# See the following for an explanation, but note this a regression example so uses MSE rather than GINI\n",
    "# https://towardsdatascience.com/feature-importance-in-decision-trees-e9450120b445\n",
    "for i in range(len(dt_model.feature_names_in_)):\n",
    "    print(f\"{dt_model.feature_names_in_[i]}: {dt_model.feature_importances_[i]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7) Do GridsearchCV to find the optimal tree and draw the tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the sklearn implementation \n",
    "from sklearn.tree import DecisionTreeRegressor, plot_tree\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    'max_depth': np.linspace(1, 50, 50, dtype='int16'),\n",
    "    'min_samples_split': np.linspace(0.1, 1.0, 10),\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(\n",
    "    estimator=DecisionTreeClassifier(criterion='gini', random_state=seed), \n",
    "    param_grid=param_grid)\n",
    "\n",
    "# Fir\n",
    "gs.fit(X=X_train, y=y_train)\n",
    "\n",
    "# Predicting the test set\n",
    "y_pred = gs.predict(X_test)\n",
    "plt.figure(figsize=(20, 20))\n",
    "plot_tree(gs.best_estimator_, feature_names=X.columns, filled=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy of the test set\n",
    "gs.score(X=X_test, y=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print 4dp of each feature importance \n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html#sklearn.tree.DecisionTreeRegressor.feature_importances_\n",
    "# See the following for an explanation, but note this a regression example so uses MSE rather than GINI\n",
    "# https://towardsdatascience.com/feature-importance-in-decision-trees-e9450120b445\n",
    "for i in range(len(gs.best_estimator_.feature_names_in_)):\n",
    "    print(f\"{gs.best_estimator_.feature_names_in_[i]}: {gs.best_estimator_.feature_importances_[i]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9) Run a LASSO model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LASSO\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we use a scikit-learn pipeline, incorporating a standard scaler, rather than just the Lasso estimator\n",
    "pipeline = Pipeline([\n",
    "    ('scaler',StandardScaler()),\n",
    "    ('model',Lasso())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92",
   "metadata": {},
   "outputs": [],
   "source": [
    "search = GridSearchCV(pipeline,\n",
    "    {'model__alpha':np.arange(0.1,10,0.1)},\n",
    "    cv = 2, scoring=\"neg_mean_squared_error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = search.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94",
   "metadata": {},
   "outputs": [],
   "source": [
    "search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95",
   "metadata": {},
   "outputs": [],
   "source": [
    "coefficients = search.best_estimator_.named_steps['model'].coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96",
   "metadata": {},
   "outputs": [],
   "source": [
    "importance = np.abs(coefficients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97",
   "metadata": {},
   "outputs": [],
   "source": [
    "importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(X.columns)[importance > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(X.columns)[importance == 0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "603a3bdf2a23e351cd3af18aabbbaca3160240b7adb15486057b82a403f7d73c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
